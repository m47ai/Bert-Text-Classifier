{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from transformers import AutoTokenizer,AutoModel\n",
    "import torch\n",
    "from torch import cuda\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "import pandas as pd \n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Intro"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this script we will fine-tune a text-clasiffier model (Multilabel/Multiclass), here we are given a pice of text/sentence/document needs to be classifed in one or more categories(multilabel) o one catgory (multiclass)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The base dataset is compose by four columns\n",
    "\n",
    "* idTask : Identity Code\n",
    "* task content 1 : Title of the article\n",
    "* idTag : Identity Code\n",
    "* tag : one of the diferent label/category\n",
    "\n",
    "\n",
    "* Tags:\n",
    "\n",
    "     * sociedad\n",
    "     * deportes \n",
    "     * politica \n",
    "     * economia\n",
    "     * clickbait\n",
    "     * cultura\n",
    "     * medio_ambiente\n",
    "     * ciencia_tecnologia\n",
    "     * educacion\n",
    "     * opinion\n",
    "\n",
    "\n",
    "\n",
    "We will use just two rows \"task content 1\"  and \"tag\", the \"tag\" column has to be change to a one-hot vector.\n",
    "\n",
    "Lets say that the label/class of a element is \"deporte\"  the model needs numeric data so it can interprete the information provided, so instead of a string we use this form type of vector: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The main objective of this function are:\n",
    "\n",
    "* Import the file in a dataframe and give it the headers as per the documentation.\n",
    "* Taking the values of all the categories and coverting it into a list.\n",
    "* The list is appened as a new column and other columns are removed. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def load_data(file_name,nrows):\n",
    "    \n",
    "    data_raw = pd.read_csv(file_name,sep = \",\")\n",
    "\n",
    "    data = data_raw.iloc[:,[1,3]]\n",
    "\n",
    "    data.columns = ['text','tag']\n",
    "\n",
    "    data['tag'].fillna('Random_Tag',inplace = True)\n",
    "\n",
    "    data.dropna(inplace = True)\n",
    "\n",
    "    data['one_hot'] = [list((row[1].values))for  row in pd.get_dummies(data['tag']).iterrows()]\n",
    "\n",
    "    if nrows > data.shape[0]:\n",
    "\n",
    "        nrows = data.shape[0]\n",
    "\n",
    "    data = data.sample(frac = nrows/data.shape[0])\n",
    "\n",
    "    return data.loc[:,['text','one_hot']]\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data = load_data(\"data.csv\",1000)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/Users/rafaelperez/.virtualenvs/q_a/lib/python3.8/site-packages/pandas/core/generic.py:6383: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._update_inplace(result)\n",
      "/Users/rafaelperez/.virtualenvs/q_a/lib/python3.8/site-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return func(*args, **kwargs)\n",
      "/Users/rafaelperez/.virtualenvs/q_a/lib/python3.8/site-packages/pandas/core/frame.py:3607: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._set_item(key, value)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hyperparameters"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We select some hyperparameters for train the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "n_classes = len(data.iloc[1,1])\n",
    "\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "\n",
    "MAX_LEN = 200\n",
    "\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "\n",
    "VALID_BATCH_SIZE = 4\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "TRAIN_SIZE = 0.8"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenizer and Model Selection\n",
    "\n",
    "We select the tokenizer and the model structure using the function from_pretained() and a model to train, here we will define the tokenizer because is necessary for creating the Pytorch Datset, we will define the model further the script"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split the data in train and validation dataset, the arguments are the  process dataframe  and the size of the test set"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def split_data(pandas_df,train_size):\n",
    "\n",
    "    if ((train_size > 0) & (train_size <=1)):\n",
    "\n",
    "        pass\n",
    "\n",
    "    elif train_size > 1:\n",
    "\n",
    "        train_size = train_size/pandas_df.shape[0]\n",
    "\n",
    "    train_set = pandas_df.sample(frac = train_size,random_state = 42)\n",
    "\n",
    "    test_set = pandas_df.drop(train_set.index).reset_index(drop = True)\n",
    "\n",
    "    train_set = train_set.reset_index(drop = True)\n",
    "\n",
    "    return train_set,test_set\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "train_dataset,test_dataset = split_data(data,0.8)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset/DataLoader\n",
    "\n",
    "We need to create a dataset that fits our needs, it's known that the deep learning models can't process raw text, so we need to pre-process the text before to send it to the neural network, also we will define a Dataloader to feed the data in bathches for training and processing \n",
    "\n",
    "Pytorch Dataset and Dataloader allow us to defining and controlling the data pre-processing and its passage to neural network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset\n",
    "\n",
    "* We will define a python class called CustomDataset, is defined to accept a list/Series/arrey of texts and labels, a tokenizer. \n",
    "\n",
    "* We will use a Bert tokenizer to encode out text data\n",
    "\n",
    "* The tokenizer uses the encode_plus method to perform tokenization and generate the necessary outputs, namely: ids, attention_mask, token_type_ids"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class CustomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, titles, targets, tokenizer, max_len):\n",
    "\n",
    "      self.titles = titles\n",
    "      self.targets = targets\n",
    "      self.tokenizer = tokenizer\n",
    "      self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "      return len(self.titles)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "\n",
    "      title = str(self.titles[item])\n",
    "\n",
    "      target = self.targets[item]\n",
    "\n",
    "      encoding = self.tokenizer.encode_plus(\n",
    "        title,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=True,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "        \n",
    "      )\n",
    "      return {\n",
    "        'review_text': title,\n",
    "        'input_ids': encoding['input_ids'].flatten(),\n",
    "        'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        'targets': torch.tensor(target, dtype=torch.long),\n",
    "        'token_type_ids': encoding['token_type_ids'].flatten()\n",
    "    }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "training_set = CustomDataset(train_dataset['text'],train_dataset['one_hot'], tokenizer, MAX_LEN)\n",
    "\n",
    "testing_set = CustomDataset(test_dataset['text'],test_dataset['one_hot'], tokenizer, MAX_LEN)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## DataLoader\n",
    "\n",
    "* Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled.\n",
    "\n",
    "* This control is achieved using the parameters such as batch_size and max_len.\n",
    "\n",
    "* Training and Validation dataloaders are used in the training and validation part of the flow respectively"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Neural Network Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* This neural network will use a BERTClass\n",
    "\n",
    "* It will be composed by a bert model, followed by a Droput Layer (to avoid overfitting) and a linear layer.\n",
    "\n",
    "* The output_1 is passed to the droput layer and the to the linear layer.\n",
    "\n",
    "* The number of output dimensions is the same as the classes/categories.\n",
    "\n",
    "* Final layer outputs is what will be used to calcuate \n",
    "the loss and to determine the accuracy of models prediction\n",
    "\n",
    "* We will initiate an instance of the network called model. This instance will be used for training and then to save the final trained model for future inference.\n",
    "\n",
    "* The Class take the parameter model_name"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class BERTClass(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,n_classes,model_name):\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.model_name = model_name\n",
    "\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = AutoModel.from_pretrained(model_name)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, n_classes)\n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "    \n",
    "        output_1= self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)['pooler_output']\n",
    "        \n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss Function\n",
    "\n",
    "* As defined above, the loss function used will be a combination of Binary Cross Entropy which is implemented as BCELogits Loss in PyTorch in case we e¡want to do a multilabel classification, if we want to do TEXT CLASIFFICATION we should use CrossEntropyLoss"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def loss_fn(function_objective,outputs, targets):\n",
    "\n",
    "    if function_objective == 'multilabel':\n",
    "\n",
    "        return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "    #elif function_objective == 'multiclass':\n",
    "\n",
    "    #    return torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "\n",
    "    else: \n",
    "\n",
    "        print('The model has to be either multilclass o multilabel, any other model will fail')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "model = BERTClass(n_classes,model_name)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at dccuchile/bert-base-spanish-wwm-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Fine-Tune\n",
    "\n",
    "Our train function trains the modle on the training set a number of times (EPOCH), each epoch is how many time complete data will be passed through the network\n",
    "\n",
    "* The dataloader passes data to the model based on the batch size.\n",
    "\n",
    "* Subsequent output from the model and the actual category are compared to calculate the loss.\n",
    "\n",
    "* Loss value is used to optimize the weights of the neurons in the network.\n",
    "\n",
    "* After every 10 steps the loss value is printed in the console."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def train(epoch,model,training_loader,device,optimizer,loss_fn):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for _,batch in enumerate(training_loader, 0):\n",
    "        \n",
    "        ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = batch['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_fn('multilabel',outputs, targets)\n",
    "\n",
    "        if _%10==0:\n",
    "\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train(epoch,model,training_loader,device,optimizer,loss_fn)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/Users/rafaelperez/.virtualenvs/q_a/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2126: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0, Loss:  0.6813402771949768\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "We save the model in the HuggingFace format"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_path = \"hg_model\"\n",
    "model.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'BERTClass' object has no attribute 'save'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9r/cr1b3d1n1_g6bj48ncxx7m5m0000gp/T/ipykernel_24481/2289706672.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"hg_model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/q_a/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1128\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[1;32m   1131\u001b[0m             type(self).__name__, name))\n\u001b[1;32m   1132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BERTClass' object has no attribute 'save'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Validation \n",
    "\n",
    "During the validation stage we pass the unseen data(Testing Dataset) to the model. This step determines how good the model performs on the unseen data.\n",
    "\n",
    "This unseen data is the 20% of train.csv which was seperated during the Dataset creation stage. During the validation stage the weights of the model are not updated. Only the final output is compared to the actual value. This comparison is then used to calcuate the accuracy of the model.\n",
    "\n",
    "As defined above to get a measure of our models performance we are using the following metrics.\n",
    "\n",
    "* Accuracy Score\n",
    "* F1 Micro\n",
    "* F1 Macro\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def validation(epoch,model,testing_loader,device,optimizer,loss_fn):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    fin_targets=[]\n",
    "    \n",
    "    fin_outputs=[]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for _, batch in enumerate(testing_loader, 0):\n",
    "    \n",
    "            ids = batch['input_ids'].to(device, dtype = torch.long)\n",
    "    \n",
    "            mask = batch['attention_mask'].to(device, dtype = torch.long)\n",
    "    \n",
    "            token_type_ids = batch['token_type_ids'].to(device, dtype = torch.long)\n",
    "    \n",
    "            targets = batch['targets'].to(device, dtype = torch.float)\n",
    "    \n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "    \n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "    \n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    \n",
    "    return fin_outputs, fin_targets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    outputs, targets = validation(epoch)\n",
    "\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    \n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    \n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    \n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "    \n",
    "    print(f\"Accuracy Score = {accuracy}\")\n",
    "    \n",
    "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    \n",
    "    print(f\"F1 Score (Macro) = {f1_score_macro}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('q_a': virtualenvwrapper)"
  },
  "interpreter": {
   "hash": "a6756d8d4a2ccb3ab7a424c8319f10ce277a1a98cac6be489aabed8f525e5ca8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}